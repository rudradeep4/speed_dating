{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 3\u001b[1;36m\n\u001b[1;33m    import matplotlib.pyplot as plt\u001b[1;36m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m\u001b[1;31m:\u001b[0m No module named 'matplotlib'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pipeline import Pipeline\n",
    "from collections import Counter\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.model_selection import KFold, GridSearchCV, train_test_split, cross_val_score\n",
    "from sklearn.metrics import r2_score\n",
    "import shap\n",
    "\n",
    "sns.set_theme(style='white', palette='Set1')\n",
    "plt.rcParams['xtick.bottom'] = False\n",
    "plt.rcParams['ytick.left'] = True\n",
    "# plt.rcParams.update({\n",
    "#     \"figure.facecolor\": (0.0, 0.0, 0.0, 0.0),\n",
    "#     \"axes.facecolor\": (0.0, 0.0, 0.0, 0.0),\n",
    "#     \"legend.facecolor\": (0.0, 0.0, 0.0, 0.0),\n",
    "#     \"savefig.facecolor\": (0.0, 0.0, 0.0, 0.0),\n",
    "# })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model social contingency using Temporal Response Functions (TRFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_an = Pipeline(\n",
    "                        trf_direction=1, \n",
    "                        trf_min_lag=0, \n",
    "                        trf_max_lag=3,\n",
    "                        regularization=1,\n",
    "                        modality='va',\n",
    "                        audio_type='auditory_nerve'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_an = pipeline_an.make_main_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame with participant responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_responses = pipeline_an.make_response_df()\n",
    "df_responses.to_csv('./df_responses.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make separate DataFrames for True and Fake trials\n",
    "df1_an = df_an[df_an['Condition'] == 'TRUE']\n",
    "df1_an = df1_an.reset_index(drop=True)\n",
    "df2_an = df_an[df_an['Condition'] != 'TRUE']\n",
    "df2_an = df2_an.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train TRF on true trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trfs = pipeline_an.train_model(df1_an)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict responses to both True & Fake trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict responses to True trials\n",
    "true_data = pipeline_an.predict_response(df1_an, trfs)\n",
    "df_trueCorrs = pipeline_an.make_trf_df(true_data)\n",
    "\n",
    "# Predict responses to Fake trials\n",
    "fake_data = pipeline_an.predict_response(df2_an, trfs)\n",
    "df_fakeCorrs = pipeline_an.make_trf_df(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataFrame with TRF predictions for trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correlations = pd.concat([df_trueCorrs, df_fakeCorrs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category(cond, resp):\n",
    "    if cond=='true' and resp=='g':\n",
    "        return 'hit'\n",
    "    elif cond=='true' and resp=='h':\n",
    "        return 'miss'\n",
    "    elif cond=='fake' and resp=='g':\n",
    "        return 'fa'\n",
    "    elif cond=='fake' and resp=='h':\n",
    "        return 'cr'\n",
    "    \n",
    "df_test = df_correlations[['listener_au', 'trial', 'condition', 'r']]\n",
    "\n",
    "response = []\n",
    "for idx, row in df_test.iterrows():\n",
    "    trials = df_responses[df_responses['VideoPath']==row['trial']]\n",
    "    mode_response = trials['Resp'].mode()[0]\n",
    "    response.append(mode_response)\n",
    "\n",
    "df_test['response'] = response\n",
    "    \n",
    "df_test['sdt'] = [get_category(row['condition'], row['response']) for i, row in df_test.iterrows()]\n",
    "df_test = df_test[df_test['listener_au']!='Pitch']\n",
    "df_test['region'] = ['eye' if row['listener_au']=='AU43' else 'mouth' for i, row in df_test.iterrows()]\n",
    "df_test['accuracy'] = [1 if ((row['condition']=='true') & (row['response']=='g')) | ((row['condition']=='fake') & (row['response']=='h')) else 0 for idx, row in df_test.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_to_resp(stim, df):\n",
    "    fits = df[df['trial']==stim]['r'].to_list()\n",
    "    return pipeline_an.aus, fits\n",
    "\n",
    "df_for_log = df_responses[df_responses['Block']=='va'].reset_index(drop=True)\n",
    "df_for_log['Condition'] = ['true' if x=='TRUE' else 'fake' for x in df_for_log['Condition']]\n",
    "df_for_log[['listener_au', 'model_performance']] = df_for_log.apply(lambda x: fit_to_resp(x['VideoPath'], df_correlations), axis=1, result_type='expand')\n",
    "df_for_log = df_for_log.explode(['listener_au', 'model_performance'])\n",
    "\n",
    "df_for_log.to_csv('./df_for_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(df_test[(df_test['listener_au']=='AU25') | (df_test['listener_au']=='AU43')], x='sdt', y='r', hue='listener_au', gap=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.barplot(df_test, x='sdt', y='r', hue='region', gap=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(10, 4))\n",
    "plt.tight_layout(w_pad=4)\n",
    "\n",
    "sns.pointplot(data=df_test[df_test['listener_au']=='AU25'], x='condition', y='r', hue='accuracy', palette=['#c10001', '#c10001'], markers=[\"o\", \"s\"], dodge=True, linestyles=['--', '-'], ax=axs[0])\n",
    "axs[0].set_ylabel('Average TRF fit')\n",
    "axs[0].set_xlabel('Condition')\n",
    "axs[0].set_title('AU25')\n",
    "\n",
    "sns.pointplot(data=df_test[df_test['listener_au']=='AU43'], x='condition', y='r', hue='accuracy', palette=['#1b6097', '#1b6097'], markers=[\"o\", \"s\"], dodge=True, linestyles=['--', '-'], ax=axs[1])\n",
    "axs[1].set_xlabel('Condition')\n",
    "axs[1].set_title('AU43')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg\n",
    "\n",
    "for au in pipeline_an.aus[:-1]:\n",
    "    a = pg.anova(data=df_test[df_test['listener_au']==au], dv='r', between=['condition', 'response'])\n",
    "    if a.iloc[2]['p-unc'] < 0.1:\n",
    "        print(au)\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot TRF for each AU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trf(\n",
    "    direction,\n",
    "    trf,\n",
    "    channel=None,\n",
    "    feature=None,\n",
    "    axes=None,\n",
    "    show=True,\n",
    "    kind=\"line\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the weights of the (forward) model across time for a select channel or feature.\n",
    "\n",
    "    Arguments:\n",
    "        channel (None | int | str): Channel selection. If None, all channels will be used. If an integer, the channel at that index will be used. If 'avg' or 'gfp' , the average or standard deviation across channels will be computed.\n",
    "        feature (None | int | str): Feature selection. If None, all features will be used. If an integer, the feature at that index will be used. If 'avg' , the average across features will be computed.\n",
    "        axes (matplotlib.axes.Axes): Axis to plot to. If None is provided (default) generate a new plot.\n",
    "        show (bool): If True (default), show the plot after drawing.\n",
    "        kind (str): Type of plot to draw. If 'line' (default), average the weights across all stimulus features, if 'image' draw a features-by-times plot where the weights are color-coded.\n",
    "\n",
    "    Returns:\n",
    "        fig (matplotlib.figure.Figure): If now axes was provided and a new figure is created, it is returned.\n",
    "    \"\"\"\n",
    "    if plt is None:\n",
    "        raise ModuleNotFoundError(\"Need matplotlib to plot TRF!\")\n",
    "    if direction == -1:\n",
    "        weights = trf.weights.T\n",
    "        print(\n",
    "            \"WARNING: decoder weights are hard to interpret, consider using the `to_forward()` method\"\n",
    "        )\n",
    "    if axes is None:\n",
    "        fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    else:\n",
    "        fig, ax = None, axes  # dont create a new figure\n",
    "    weights = trf.weights\n",
    "    # select channel and or feature\n",
    "    if weights.shape[0] == 1:\n",
    "        feature = 0\n",
    "    if weights.shape[-1] == 1:\n",
    "        channel = 0\n",
    "    if channel is None and feature is None:\n",
    "        raise ValueError(\"You must specify a subset of channels or features!\")\n",
    "    if feature is not None:\n",
    "        image_ylabel = \"channel\"\n",
    "        if isinstance(feature, int):\n",
    "            weights = weights[feature, :, :]\n",
    "        elif feature == \"avg\":\n",
    "            weights = weights.mean(axis=0)\n",
    "        else:\n",
    "            raise ValueError('Argument `feature` must be an integer or \"avg\"!')\n",
    "    if channel is not None:\n",
    "        image_ylabel = \"feature\"\n",
    "        if isinstance(channel, int):\n",
    "            weights = weights.T[channel].T\n",
    "        elif channel == \"avg\":\n",
    "            weights = weights.mean(axis=-1)\n",
    "        elif channel == \"gfp\":\n",
    "            weights = weights.std(axis=-1)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                'Argument `channel` must be an integer, \"avg\" or \"gfp\"'\n",
    "            )\n",
    "        weights = weights.T  # transpose so first dimension is time\n",
    "    # plot the result\n",
    "    scaler = StandardScaler()\n",
    "    # normalizer = MinMaxScaler()\n",
    "    if kind == \"line\":\n",
    "        ax.plot(\n",
    "            trf.times.flatten(), scaler.fit_transform(weights.reshape(-1, 1)), linewidth=2 - 0.01 * weights.shape[-1]\n",
    "        )\n",
    "        ax.set(\n",
    "            xlabel=\"Time lag[s]\",\n",
    "            ylabel=\"Amplitude [a.u.]\",\n",
    "            xlim=(trf.times.min(), trf.times.max()),\n",
    "        )\n",
    "    elif kind == \"image\":\n",
    "        scale = trf.times.max() / len(trf.times)\n",
    "        im = ax.imshow(\n",
    "            weights.T,\n",
    "            origin=\"lower\",\n",
    "            aspect=\"auto\",\n",
    "            extent=[0, weights.shape[0], 0, weights.shape[1]],\n",
    "        )\n",
    "        extent = np.asarray(im.get_extent(), dtype=float)\n",
    "        extent[:2] *= scale\n",
    "        im.set_extent(extent)\n",
    "        ax.set(\n",
    "            xlabel=\"Time lag [s]\",\n",
    "            ylabel=image_ylabel,\n",
    "            xlim=(trf.times.min(), trf.times.max()),\n",
    "        )\n",
    "    if show is True:\n",
    "        plt.show()\n",
    "    if fig is not None:\n",
    "        return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplots_adjust(hspace=1, wspace=0.5)\n",
    "for i, au_id in enumerate(pipeline_an.aus):\n",
    "    ax = plt.subplot(4, 3, i + 1)\n",
    "    plot_trf(direction=1, trf=trfs[i], axes=ax, show=False) \n",
    "    ax.set_title(f'TRF for {au_id}')\n",
    "    if (i==0) or (i==6) or (i==6) or (i==7) or (i==8) or (i==10):\n",
    "        ax.get_lines()[0].set_color(\"g\")\n",
    "    else:\n",
    "        ax.get_lines()[0].set_color(\"k\")\n",
    "    ax.get_lines()[0].set_linewidth(2)\n",
    "    ax.set_ylabel('')\n",
    "    ax.set_ylim(-2.5, 2.5)\n",
    "    ax.axhline(y=0, color='k', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare which similarity metric to use for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options: r, r2, mae, mse, rmse\n",
    "similarity_measure = 'r'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare TRF prediction accuracy for True & Fake trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.boxplot(data=df_correlations, x='listener_au', y=similarity_measure, hue='condition', gap=0.2, fill=False)\n",
    "sns.stripplot(data=df_correlations, x='listener_au', y=similarity_measure, hue='condition', ax=ax, dodge=True, alpha=0.1, legend=False)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Action Unit')\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Ridge plot\n",
    "# with sns.axes_style(rc={\"axes.facecolor\": (0, 0, 0, 0)}):\n",
    "#     g = sns.FacetGrid(df_correlations, row=\"listener_au\", hue=\"condition\", aspect=10, height=0.5, palette='Set2', legend_out=True)\n",
    "#     g.map(sns.kdeplot, similarity_measure, bw_adjust=.5, clip_on=False, fill=True, lw=1, alpha=0.5)\n",
    "#     g.map(sns.kdeplot, similarity_measure, bw_adjust=.5, clip_on=False, lw=0.5)\n",
    "#     g.refline(x=0, lw=0.5, alpha=1, linestyle=\"--\", clip_on=False)\n",
    "#     g.refline(y=0, lw=1, alpha=1, linestyle=\"-\", clip_on=False)\n",
    "#     for i, ax in enumerate(g.axes.flat):\n",
    "#         ax.text(0, .2, pipeline_an.aus[i], fontsize=8, fontweight=\"bold\", ha=\"left\", va=\"center\", color=ax.lines[-1].get_color(), transform=ax.transAxes)\n",
    "#     g.figure.subplots_adjust(hspace=-.3)\n",
    "#     g.set_titles(\"\")\n",
    "#     g.set(yticks=[], ylabel=\"\")\n",
    "#     g.despine(left=True, bottom=True)\n",
    "#     g.set_axis_labels(similarity_measure, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-test between TRF prediction accuracy for True & Fake trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_r, p_r = pipeline_an.ttests(df_correlations, 'r')\n",
    "t_r2, p_r2 = pipeline_an.ttests(df_correlations, 'r2')\n",
    "t_mae, p_mae = pipeline_an.ttests(df_correlations, 'mae')\n",
    "t_mse, p_mse = pipeline_an.ttests(df_correlations, 'mse')\n",
    "t_rmse, p_rmse = pipeline_an.ttests(df_correlations, 'rmse')\n",
    "\n",
    "num_trues = len(df_correlations[df_correlations['condition']=='true']) // len(pipeline_an.aus)\n",
    "num_fakes = len(df_correlations[df_correlations['condition']=='fake']) // len(pipeline_an.aus)\n",
    "\n",
    "df_ttest = pd.DataFrame({\n",
    "    'listener_au': np.tile(pipeline_an.aus, 5),\n",
    "    'metric': np.repeat(['r', 'r2', 'mae', 'mse', 'rmse'], len(pipeline_an.aus)),\n",
    "    't': np.concatenate((t_r, t_r2, t_mae, t_mse, t_rmse), axis=None),\n",
    "    'p': np.concatenate((p_r, p_r2, p_mae, p_mse, p_rmse), axis=None),\n",
    "    'image': np.tile(pipeline_an.au_gifs, 5)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['green' if p < 0.05 else 'black' for p in df_ttest[df_ttest['metric']=='r'].p]\n",
    "sns.barplot(x='listener_au', y='t', data=df_ttest[df_ttest['metric']=='r'], palette=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.barplot(data=df_ttest, x='listener_au', y='t', hue='metric')\n",
    "cols = ['green' if p < 0.05 else 'black' for p in df_ttest.p]\n",
    "\n",
    "g = sns.FacetGrid(df_ttest, col='metric', aspect=2, sharex=True, sharey=True, despine=False, col_wrap=2)\n",
    "g.map_dataframe(sns.barplot, x='listener_au', y='t', palette=cols)\n",
    "plt.xticks(rotation=90)\n",
    "plt.xlabel('Action Unit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alt.renderers.enable(\"mimetype\")\n",
    "\n",
    "# alt.Chart(df_ttest).mark_bar().encode(\n",
    "#     y = alt.Y('t', aggregate='mean').axis(\n",
    "#         title=\"t_statistic\",\n",
    "#         titleAngle=0,\n",
    "#         titleAlign=\"left\",\n",
    "#         titleY=-2,\n",
    "#         titleX=0,\n",
    "#     ),\n",
    "#     x = alt.X('listener_au', axis=alt.Axis(labelAngle=-90)).title('Action Unit'),\n",
    "#     tooltip = ['p', 'image'],\n",
    "#     row = 'metric',\n",
    "#     color = alt.condition(alt.datum.p < 0.05, alt.value('green'), alt.value('white'))\n",
    "# ).configure_axis(grid=False).configure_view(stroke=None).properties(width=350, height=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare model accuracy with participant ratings of genuineness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = []\n",
    "trial_cond = []\n",
    "listener_au = []\n",
    "duration = []\n",
    "dyad = []\n",
    "times_presented = []\n",
    "model_performance = []\n",
    "subject_accuracy = []\n",
    "genuineness = []\n",
    "confidence = []\n",
    "\n",
    "for idx, row in df_correlations.iterrows():\n",
    "    if any(df_responses['VideoPath'] == row['trial']):\n",
    "        ab = df_responses.loc[df_responses['VideoPath'] == row['trial']]\n",
    "\n",
    "        subject_accuracy.append(Counter(ab['Accuracy'])[True]/len(ab))\n",
    "        genuineness.append(Counter(ab['Resp'])['g']/len(ab))\n",
    "        confidence.append(sum(ab['LikertResp'])/len(ab))\n",
    "        trial.append(row['trial'])\n",
    "        duration.append(row['duration'])\n",
    "        trial_cond.append(row['condition'])\n",
    "        listener_au.append(row['listener_au'])\n",
    "        dyad.append(row['displayed_dyad'])\n",
    "        times_presented.append(len(df_responses.loc[df_responses['VideoPath'] == row['trial']]))\n",
    "        model_performance.append(row['r'])\n",
    "\n",
    "df_regression = pd.DataFrame({\n",
    "                                'trial': trial,\n",
    "                                'condition': trial_cond,\n",
    "                                'listener_au': listener_au,\n",
    "                                'duration': duration,\n",
    "                                'dyad': dyad,\n",
    "                                'times_presented': times_presented,\n",
    "                                'model_performance': model_performance,\n",
    "                                'subject_accuracy': subject_accuracy,\n",
    "                                'confidence': confidence,\n",
    "                                'genuineness': genuineness\n",
    "                            })\n",
    "df_regression.to_csv('./df_regression.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression plots of model 'performance' & participant ratings of genuineness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.lmplot(data=df_regression, y='model_performance', x='subject_accuracy', hue='condition', col='listener_au', col_wrap=2, height=3, aspect=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=df_regression, y='genuineness', x='model_performance', hue='condition', col='listener_au', col_wrap=4, height=3, aspect=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.lmplot(data=df_regression, x='model_performance', y='genuineness', col='listener_au', col_wrap=4, height=3, aspect=1, scatter_kws=dict(color=\"k\", alpha=0.3), line_kws=dict(color=\"k\"))\n",
    "\n",
    "def annotate(data, **kws):\n",
    "\tr, p = stats.pearsonr(data['genuineness'], data['model_performance'])\n",
    "\tr2 = r2_score(data['genuineness'], data['model_performance'],)\n",
    "\tax = plt.gca()\n",
    "\tax.text(.01, .8, 'r2={:.2f}, \\np={:.2g}'.format(r2, p), transform=ax.transAxes)\n",
    "\t\n",
    "g.map_dataframe(annotate)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(metric):\n",
    "    feature_vector = []\n",
    "    target_vector = []\n",
    "\n",
    "    grouped_trials = df_correlations.groupby('trial')\n",
    "\n",
    "    for name, trial in grouped_trials:\n",
    "        target_vector.append(trial['condition'].to_list()[0])\n",
    "        feature_vector.append(trial[metric].to_list())\n",
    "\n",
    "    feature_vector = np.asarray(feature_vector)\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(feature_vector)\n",
    "\n",
    "    target_vector = np.asarray(target_vector)\n",
    "\n",
    "    return scaled_features, target_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv(features, targets):\n",
    "\tN_TRIALS = 10\n",
    "\tscores = np.zeros(N_TRIALS)\n",
    "\n",
    "\tsvc = SVC(probability=True)\n",
    "\tparam_grid = [\n",
    "\t\t{'C': np.logspace(-5, 3, 9), 'kernel':['linear']},\n",
    "\t\t# {'C': np.logspace(-5, 3, 9), 'gamma': np.logspace(-5, 2, 8), 'kernel':['rbf']}\n",
    "\t]\n",
    "\n",
    "\tfor i in range(N_TRIALS):\n",
    "\t\tinner_cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "\t\touter_cv = KFold(n_splits=3, shuffle=True, random_state=i)\n",
    "\n",
    "\t\tmodel = GridSearchCV(estimator=svc, param_grid=param_grid, cv=inner_cv, n_jobs=-1)\n",
    "\t\tmodel.fit(features, targets)\n",
    "\n",
    "\t\tscore = cross_val_score(model, features, targets, cv=outer_cv, n_jobs=-1)\n",
    "\t\tscores[i] = score.mean()\n",
    "\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get best models for each similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics = pd.DataFrame({})\n",
    "\n",
    "models = {'r': [], 'r2': [], 'mae': [], 'mse': [], 'rmse': []}\n",
    "\n",
    "for metric in models.keys():\n",
    "    features, targets = get_training_data(metric)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.5, random_state=1)\n",
    "\n",
    "    results = nested_cv(features, targets)\n",
    "    df_metrics[metric] = results\n",
    "\n",
    "    svc = SVC(probability=True)\n",
    "    param_grid = [{'C': np.logspace(-5, 3, 9), 'kernel':['linear']}]\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "    model = GridSearchCV(estimator=svc, param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    optimal_params = model.best_params_\n",
    "\n",
    "    final_clf = SVC(C=optimal_params.get('C'), kernel=optimal_params.get('kernel'), probability=True)\n",
    "    final_clf.fit(X_train, y_train)\n",
    "    # final_clf.score(X_test, y_test)\n",
    "    models[metric].append(final_clf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfm = df_metrics.melt()\n",
    "dfm.groupby('variable').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot average accuracy (over K folds) for each similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='variable', y='value', data=dfm[dfm['variable']=='r'], fill=False, width=0.25)\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('')\n",
    "plt.ylabel('SVM Accuracy')\n",
    "plt.xticks([])\n",
    "plt.axhline(y=0.5, color='k', ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='variable', y='value', data=dfm, fill=True)\n",
    "plt.xlabel('Similarity Metric')\n",
    "plt.ylabel('Average SVM accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### t-tests for above chance accuracy for each similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_results =  [\n",
    "                    stats.ttest_1samp(dfm[dfm['variable']=='r']['value'], 0.5),\n",
    "                    stats.ttest_1samp(dfm[dfm['variable']=='r2']['value'], 0.5),\n",
    "                    stats.ttest_1samp(dfm[dfm['variable']=='mae']['value'], 0.5),\n",
    "                    stats.ttest_1samp(dfm[dfm['variable']=='mse']['value'], 0.5),\n",
    "                    stats.ttest_1samp(dfm[dfm['variable']=='rmse']['value'], 0.5)\n",
    "                ]\n",
    "\n",
    "print(\"r: \", stat_results[0])\n",
    "print(\"r2: \", stat_results[1])\n",
    "print(\"mae: \", stat_results[2])\n",
    "print(\"mse: \", stat_results[3])\n",
    "print(\"rmse: \", stat_results[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapley values on SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "features, targets = get_training_data('r')\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0: r\n",
    "# 1: r2\n",
    "# 2: mae\n",
    "# 3: mse\n",
    "# 4: rmse\n",
    "model_metric_idx = 'r'\n",
    "\n",
    "model_labels = models[model_metric_idx][0].classes_\n",
    "true_label_idx = np.argwhere(model_labels=='true')[0][0]\n",
    "fake_label_idx = np.argwhere(model_labels=='fake')[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(models[model_metric_idx][0].predict_proba, X_train, feature_names=pipeline_an.aus)\n",
    "shap_values = explainer(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values[:, :, true_label_idx], max_display=11, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values[:, :, fake_label_idx], max_display=11, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = SVR(kernel='linear', C=0.1, epsilon=0.2)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplots_adjust(hspace=1, wspace=0.5)\n",
    "\n",
    "for i, au_id in enumerate(pipeline_an.aus):\n",
    "    accuracy_true = df_regression[(df_regression['listener_au'] == au_id) & (df_regression['condition'] == 'true')]['model_performance'].to_numpy().reshape(-1, 1)\n",
    "    scaler_true = StandardScaler()\n",
    "    accuracy_true_scaled = scaler_true.fit_transform(accuracy_true)\n",
    "\n",
    "    accuracy_fake = df_regression[(df_regression['listener_au'] == au_id) & (df_regression['condition'] == 'fake')]['model_performance'].to_numpy().reshape(-1, 1)\n",
    "    scaler_fake = StandardScaler()\n",
    "    accuracy_fake_scaled = scaler_fake.fit_transform(accuracy_fake)\n",
    "\n",
    "    genuineness_true = df_regression[(df_regression['listener_au'] == au_id) & (df_regression['condition'] == 'true')]['genuineness'].to_numpy()\n",
    "    genuineness_fake = df_regression[(df_regression['listener_au'] == au_id) & (df_regression['condition'] == 'fake')]['genuineness'].to_numpy()\n",
    "\n",
    "    ax = plt.subplot(4, 3, i+1)\n",
    "    sns.regplot(y=regr.fit(accuracy_true_scaled, genuineness_true).predict(accuracy_true_scaled), x=accuracy_true_scaled, ax=ax, label='True')\n",
    "    sns.regplot(y=regr.fit(accuracy_fake_scaled, genuineness_fake).predict(accuracy_fake_scaled), x=accuracy_fake_scaled, ax=ax, label='Fake')\n",
    "    ax.set_title(au_id)\n",
    "    ax.set_xlabel('model performance')\n",
    "    ax.set_ylabel('genuineness')\n",
    "    ax.legend(frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data_svr():\n",
    "       feature_vector = []\n",
    "       target_vector = []\n",
    "       \n",
    "       grouped_trials = df_regression.groupby('trial')\n",
    "       \n",
    "       for name, trial in grouped_trials:\n",
    "             target_vector.append(trial['genuineness'].to_list()[0])\n",
    "             feature_vector.append(trial['model_performance'].to_list())\n",
    "\n",
    "       feature_vector = np.asarray(feature_vector)\n",
    "       scaler = StandardScaler()\n",
    "       scaled_features = scaler.fit_transform(feature_vector)\n",
    "\n",
    "       target_vector = np.asarray(target_vector)\n",
    "      #  target_scaler = StandardScaler()\n",
    "      #  scaled_targets = target_scaler.fit_transform(target_vector.reshape(-1, 1))\n",
    "\n",
    "       return scaled_features, target_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nested_cv_svr(features, targets):\n",
    "\tN_TRIALS = 20\n",
    "\tscores = np.zeros(N_TRIALS)\n",
    "\n",
    "\tsvr = SVR()\n",
    "\tparam_grid = [\n",
    "\t\t{'C': np.logspace(-5, 3, 9), 'kernel':['linear']},\n",
    "\t\t{'C': np.logspace(-5, 3, 9), 'gamma': np.logspace(-5, 2, 8), 'kernel':['rbf']}\n",
    "\t]\n",
    "\n",
    "\tfor i in range(N_TRIALS):\n",
    "\t\tinner_cv = KFold(n_splits=5, shuffle=True, random_state=i)\n",
    "\t\touter_cv = KFold(n_splits=3, shuffle=True, random_state=i)\n",
    "\n",
    "\t\tmodel = GridSearchCV(estimator=svr, param_grid=param_grid, cv=inner_cv, n_jobs=-1)\n",
    "\t\tmodel.fit(X_train, y_train)\n",
    "\n",
    "\t\ttest_score = cross_val_score(model, features, targets, cv=outer_cv, n_jobs=-1)\n",
    "\t\tscores[i] = test_score.mean()\n",
    "\n",
    "\treturn scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, targets = get_training_data_svr()\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.5, random_state=1)\n",
    "\n",
    "scores = nested_cv_svr(features, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVR()\n",
    "param_grid = [{'C': np.logspace(-5, 3, 9), 'kernel':['linear']}]\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "model = GridSearchCV(estimator=svc, param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "model.fit(X_train, y_train)\n",
    "best_model = model.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clf = SVR(C=model.best_params_.get('C'), kernel='linear')\n",
    "final_clf.fit(X_train, y_train)\n",
    "final_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(best_model.predict, X_train, feature_names=pipeline_an.aus)\n",
    "shap_values = explainer(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, max_display=11)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
