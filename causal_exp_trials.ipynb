{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pipeline import Pipeline\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get model performance and genuineness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_an = Pipeline(\n",
    "                        trf_direction=1, \n",
    "                        trf_min_lag=0, \n",
    "                        trf_max_lag=3,\n",
    "                        regularization=1,\n",
    "                        modality='va',\n",
    "                        audio_type='auditory_nerve'\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_an = pipeline_an.make_main_df()\n",
    "df_responses = pipeline_an.make_response_df()\n",
    "\n",
    "# Make separate DataFrames for True and Fake trials\n",
    "df1_an = df_an[df_an['Condition'] == 'TRUE']\n",
    "df1_an = df1_an.reset_index(drop=True)\n",
    "df2_an = df_an[df_an['Condition'] != 'TRUE']\n",
    "df2_an = df2_an.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trfs = pipeline_an.train_model(df1_an)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('trfs.pkl', 'wb') as file:\n",
    "    pickle.dump(trfs, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict responses to True trials\n",
    "true_data = pipeline_an.predict_response(df1_an, trfs)\n",
    "df_trueCorrs = pipeline_an.make_trf_df(true_data)\n",
    "\n",
    "# Predict responses to Fake trials\n",
    "fake_data = pipeline_an.predict_response(df2_an, trfs)\n",
    "df_fakeCorrs = pipeline_an.make_trf_df(fake_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_correlations = pd.concat([df_trueCorrs, df_fakeCorrs])\n",
    "df_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = []\n",
    "trial_cond = []\n",
    "listener_au = []\n",
    "duration = []\n",
    "dyad = []\n",
    "times_presented = []\n",
    "model_performance = []\n",
    "subject_accuracy = []\n",
    "genuineness = []\n",
    "confidence = []\n",
    "\n",
    "for idx, row in df_correlations.iterrows():\n",
    "    if any(df_responses['VideoPath'] == row['trial']):\n",
    "        ab = df_responses.loc[df_responses['VideoPath'] == row['trial']]\n",
    "\n",
    "        subject_accuracy.append(Counter(ab['Accuracy'])[True]/len(ab))\n",
    "        genuineness.append(Counter(ab['Resp'])['g']/len(ab))\n",
    "        confidence.append(sum(ab['LikertResp'])/len(ab))\n",
    "        trial.append(row['trial'])\n",
    "        duration.append(row['duration'])\n",
    "        trial_cond.append(row['condition'])\n",
    "        listener_au.append(row['listener_au'])\n",
    "        dyad.append(row['displayed_dyad'])\n",
    "        times_presented.append(len(df_responses.loc[df_responses['VideoPath'] == row['trial']]))\n",
    "        model_performance.append(row['r'])\n",
    "\n",
    "df_regression = pd.DataFrame({\n",
    "                                'trial': trial,\n",
    "                                'condition': trial_cond,\n",
    "                                'listener_au': listener_au,\n",
    "                                'duration': duration,\n",
    "                                'dyad': dyad,\n",
    "                                'times_presented': times_presented,\n",
    "                                'model_performance': model_performance,\n",
    "                                'subject_accuracy': subject_accuracy,\n",
    "                                'confidence': confidence,\n",
    "                                'genuineness': genuineness\n",
    "                            })\n",
    "df_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentile(df, au, col, percent):\n",
    "    return df[df['listener_au']==au][col].quantile(percent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_description(df, au, col):\n",
    "    return df[df['listener_au']==au][col].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_eyes = df[\n",
    "    (df['condition']=='true') & \n",
    "    (df['listener_au']=='AU43') & \n",
    "    (df['genuineness'] > get_percentile(df, 'AU43', 'genuineness', .70)) & \n",
    "    (df['model_performance'] > get_percentile(df, 'AU43', 'model_performance', .70))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AU17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_17 = df[\n",
    "    (df['condition']=='true') & \n",
    "    (df['listener_au']=='AU17') & \n",
    "    (df['genuineness'] > get_percentile(df, 'AU17', 'genuineness', .70)) & \n",
    "    (df['model_performance'] > get_percentile(df, 'AU17', 'model_performance', .70))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_17 = pd.merge(hit_17, hit_eyes, 'inner', on='trial')\n",
    "# hit_17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AU25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_25 = df[\n",
    "    (df['condition']=='true') & \n",
    "    (df['listener_au']=='AU25') & \n",
    "    (df['genuineness'] > get_percentile(df, 'AU25', 'genuineness', .70)) & \n",
    "    (df['model_performance'] > get_percentile(df, 'AU25', 'model_performance', .70))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hit_25 = pd.merge(hit_25, hit_eyes, 'inner', on='trial')\n",
    "# hit_25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hit = pd.concat([hit_17, hit_25]).drop_duplicates(subset='trial').reset_index(drop=True)\n",
    "df_hit['sdt'] = np.repeat(['hit'], len(df_hit))\n",
    "df_hit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Misses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_eyes = df[\n",
    "    (df['condition']=='true') & \n",
    "    (df['listener_au']=='AU43') & \n",
    "    (df['genuineness'] < get_percentile(df, 'AU43', 'genuineness', .41)) & \n",
    "    (df['model_performance'] < get_percentile(df, 'AU43', 'model_performance', .60))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AU17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_17 = df[\n",
    "    (df['condition']=='true') & \n",
    "    (df['listener_au']=='AU17') & \n",
    "    (df['genuineness'] < get_percentile(df, 'AU17', 'genuineness', .41)) & \n",
    "    (df['model_performance'] > get_percentile(df, 'AU17', 'model_performance', .60))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_17 = pd.merge(miss_17, miss_eyes, 'inner', on='trial')\n",
    "# miss_17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AU25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_25 = df[\n",
    "    (df['condition']=='true') & \n",
    "    (df['listener_au']=='AU25') & \n",
    "    (df['genuineness'] < get_percentile(df, 'AU25', 'genuineness', .41)) & \n",
    "    (df['model_performance'] > get_percentile(df, 'AU25', 'model_performance', .60))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miss_25 = pd.merge(miss_25, miss_eyes, 'inner', on='trial')\n",
    "# miss_25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_miss = pd.concat([miss_17, miss_25]).drop_duplicates(subset='trial').reset_index(drop=True)\n",
    "df_miss['sdt'] = np.repeat(['miss'], len(df_miss))\n",
    "df_miss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct Rejections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_eyes = df[\n",
    "    (df['condition']=='fake') & \n",
    "    (df['listener_au']=='AU43') & \n",
    "    (df['genuineness'] < get_percentile(df, 'AU43', 'genuineness', .35)) & \n",
    "    (df['model_performance'] > get_percentile(df, 'AU43', 'model_performance', .70))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AU17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_17 = df[\n",
    "    (df['condition']=='fake') & \n",
    "    (df['listener_au']=='AU17') & \n",
    "    (df['genuineness'] < get_percentile(df, 'AU17', 'genuineness', .35)) & \n",
    "    (df['model_performance'] < get_percentile(df, 'AU17', 'model_performance', .30))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_17 = pd.merge(cr_17, cr_eyes, 'inner', on='trial')\n",
    "# cr_17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AU25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_25 = df[\n",
    "    (df['condition']=='fake') & \n",
    "    (df['listener_au']=='AU25') & \n",
    "    (df['genuineness'] < get_percentile(df, 'AU25', 'genuineness', .35)) & \n",
    "    (df['model_performance'] < get_percentile(df, 'AU25', 'model_performance', .30))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_25 = pd.merge(cr_25, cr_eyes, 'inner', on='trial')\n",
    "# cr_25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cr = pd.concat([cr_17, cr_25]).drop_duplicates(subset='trial').reset_index(drop=True)\n",
    "df_cr['sdt'] = np.repeat(['cr'], len(df_cr))\n",
    "df_cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# False Alarms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_eyes = df[\n",
    "    (df['condition']=='fake') & \n",
    "    (df['listener_au']=='AU43') & \n",
    "    (df['genuineness'] > get_percentile(df, 'AU43', 'genuineness', .50)) & \n",
    "    (df['model_performance'] < get_percentile(df, 'AU43', 'model_performance', .50))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AU17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_17 = df[\n",
    "    (df['condition']=='fake') & \n",
    "    (df['listener_au']=='AU17') & \n",
    "    (df['genuineness'] > get_percentile(df, 'AU17', 'genuineness', .50)) & \n",
    "    (df['model_performance'] > get_percentile(df, 'AU17', 'model_performance', .50))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_17 = pd.merge(fa_17, fa_eyes, 'inner', on='trial')\n",
    "# fa_17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AU25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_25 = df[\n",
    "    (df['condition']=='fake') & \n",
    "    (df['listener_au']=='AU25') & \n",
    "    (df['genuineness'] > get_percentile(df, 'AU25', 'genuineness', .50)) & \n",
    "    (df['model_performance'] > get_percentile(df, 'AU25', 'model_performance', .50))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa_25 = pd.merge(fa_25, fa_eyes, 'inner', on='trial')\n",
    "# fa_25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fa = pd.concat([fa_17, fa_25]).drop_duplicates(subset='trial').reset_index(drop=True)\n",
    "df_fa['sdt'] = np.repeat(['fa'], len(df_fa))\n",
    "df_fa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final trial list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.concat([df_hit, df_miss, df_cr, df_fa]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_dict = {'trial': 'video_path', 'condition_x': 'condition', 'duration_x': 'duration', 'dyad_x': 'dyad', 'model_performance_x': 'mouth_fit', 'model_performance_y': 'eye_fit', 'genuineness_x': 'genuineness', 'sdt': 'category'}\n",
    "\n",
    "df_final = df_final.rename(columns=selector_dict)[[*selector_dict.values()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = np.tile(np.asarray(['original', 'mouth', 'eyes', 'nods']).reshape(1, -1), (len(df_final), 1))\n",
    "df_final['block'] = list(block)\n",
    "df_final = df_final.explode(column='block')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['video_path'] = df_final.apply(lambda x: x['video_path'].replace(os.sep, '/'), axis=1)\n",
    "\n",
    "df_final['audio_path'] = df_final.apply(lambda x: f\"./stimuli/audio/{x['dyad']}/{x['video_path'].split('/')[5][:-7]}_audio.wav\", axis=1)\n",
    "df_final['video_path'] = df_final.apply(lambda x: f\"./stimuli/{x['block']}/{'_'.join(x['video_path'].split('/')[4:])[:-7]}.mov\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = [\n",
    "            './stimuli/audio/11/8_audio.wav', './stimuli/audio/24/4_audio.wav',     # misses\n",
    "            './stimuli/audio/49/2_5_audio.wav', './stimuli/audio/11/1_2_audio.wav', './stimuli/audio/18/1_3_audio.wav'      # correct rejections\n",
    "        ]\n",
    "\n",
    "df_practice = df_final[df_final['audio_path'].isin(exclude)].reset_index(drop=True)\n",
    "df_practice\n",
    "\n",
    "df_practice.to_csv('./practice_trials.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final[~df_final['audio_path'].isin(exclude)].reset_index(drop=True)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('./trial_list.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
